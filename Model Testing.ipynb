{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73135157-35fc-4f81-9970-ee1dd2629234",
   "metadata": {},
   "source": [
    "# Data Mining (CpSc 8650) Course Project  Quality Evaluation of Skull Stripped Brain MRI Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa61c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import random\n",
    "import csv\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f38ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and return data \n",
    "def load_data(csv_file_path, test_size = 0.3, x_names = [], y_names = []):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Load training data and split it into training and validation set\n",
    "    \"\"\"\n",
    "    print (\"Loading data from: \",csv_file_path)\n",
    "    #reads CSV file into a single dataframe variable\n",
    "    data_df = pd.read_csv(csv_file_path, names=x_names+y_names, skiprows=1)\n",
    "\n",
    "    #yay dataframes, we can select rows and columns by their names\n",
    "    X = data_df[x_names].values\n",
    "    #and our steering commands as our output data\n",
    "    y = data_df[y_names ].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 64\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def process_scan(path):\n",
    "    \n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    #Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    \n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "68a0d543-e1dc-4e17-bcaa-84de77a0b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [0,-5,5,-10,10,15,-15]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, labels):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, labels\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3db25cb-dbba-4d77-b1dc-23effcff6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from:  D:\\OneDrive - Clemson University\\Clemson Courses\\Data mining\\Project\\Project\\dataset\\Label_file.csv\n"
     ]
    }
   ],
   "source": [
    "label_file_path = os.path.join(os.path.abspath(os.pardir), \"dataset\", \"Label_file.csv\")\n",
    "X, y = load_data(label_file_path, x_names = [\"Filename\"], y_names = [\"Recognizable-Facial-Feature\",\"Brain-Feature-Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87860c1b-3377-4c3f-baab-212b27c07585",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y=='Yes'] = True\n",
    "y[y=='No'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75b7c0ca-9bc3-41e5-8123-79a73457a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(width=128, height=128, depth=64):\n",
    "        \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "        inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "        x = layers.Conv3D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.1)(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.2)(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "        outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
    "\n",
    "        # Define the model.\n",
    "        model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "335dd5c8-7e11-4c20-a635-4f93b8c812e2",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1\n",
      "Data Loaded\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-30799d9d787f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Loaded\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;31m# Define data loaders.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0mvalidation_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_valid_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    779\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m     \"\"\"\n\u001b[0;32m--> 781\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    782\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4659\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4660\u001b[0m     \u001b[0;34m\"\"\"See `Dataset.from_tensor_slices()` for details.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4661\u001b[0;31m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4662\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4663\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mnormalize_element\u001b[0;34m(element, element_signature)\u001b[0m\n\u001b[1;32m    127\u001b[0m           \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"dtype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m           normalized_components.append(\n\u001b[0;32m--> 129\u001b[0;31m               ops.convert_to_tensor(t, name=\"component_%d\" % i, dtype=dtype))\n\u001b[0m\u001b[1;32m    130\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack_as\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/profiler/trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[1;32m   1619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1620\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1621\u001b[0;31m       \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1622\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1623\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[0;31m# Unused.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m    271\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0;32m--> 272\u001b[0;31m                         allow_broadcast=True)\n\u001b[0m\u001b[1;32m    273\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tf.constant\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m   \u001b[0;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m   \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/software/venv/tf_gpu/lib/python3.7/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "skf = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "skf.get_n_splits(X, y)\n",
    "fold = 0\n",
    "\n",
    "model = get_model(width=128, height=128, depth=64)\n",
    "for train_index, test_index in skf.split(X):\n",
    "    fold+=1\n",
    "    print(\"Fold:\"+str(fold))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    iteration=0\n",
    "    #print(len(y_train), len(y_test))\n",
    "    # X_train_data = np.array([process_scan(os.path.join(os.path.abspath(os.pardir), \"dataset\", \"files\", path[0]+\".gz\")) for path in X_train])\n",
    "    # X_valid_data = np.array([process_scan(os.path.join(os.path.abspath(os.pardir), \"dataset\", \"files\", path[0]+\".gz\")) for path in X_test])\n",
    "    \n",
    "    outfileTrain = \"Fold \" + str(fold) + \" Checkpoints.pkl\"\n",
    "    outfileTest = \"Fold \" + str(fold) + \" Checkpoints test set.pkl\"\n",
    "    #X_train_valid = [X_valid_data]\n",
    "#     with open(outfile, \"wb\") as f:\n",
    "#         pickle.dump(X_train_valid, f)\n",
    "    #print(len(X_valid_data), len(y_test))\n",
    "    # Load the objects back\n",
    "    with open(outfileTrain, \"rb\") as f:\n",
    "        [X_train_data, abc] = pickle.load(f)\n",
    "        \n",
    "    with open(outfileTest, \"rb\") as f:\n",
    "        [X_valid_data] = pickle.load(f)\n",
    "    print(\"Data Loaded\")\n",
    "    # Define data loaders.\n",
    "    train_loader = tf.data.Dataset.from_tensor_slices((X_train_data,y_train.astype(\"float32\")))\n",
    "    validation_loader = tf.data.Dataset.from_tensor_slices((X_valid_data, y_test.astype(\"float32\")))\n",
    "    \n",
    "    batch_size = 24\n",
    "    # Augment the on the fly during training.\n",
    "    train_dataset = (\n",
    "        train_loader.shuffle(len(X_train_data))\n",
    "        .map(train_preprocessing)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "    \n",
    "    # Only rescale.\n",
    "    validation_dataset = (\n",
    "        validation_loader.shuffle(len(X_valid_data))\n",
    "        .map(validation_preprocessing)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "    # Build model.\n",
    "    \n",
    "    # Compile model.\n",
    "    initial_learning_rate = 0.00001\n",
    "    lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Define callbacks.\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        \"3d_image_classification_relu.h5\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "    # Train the model, doing validation at the end of each epoch\n",
    "    epochs = 100\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb],\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "        ax[i].plot(model.history.history[metric])\n",
    "        ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "        ax[i].set_title(\"Model {}\".format(metric))\n",
    "        ax[i].set_xlabel(\"epochs\")\n",
    "        ax[i].set_ylabel(metric)\n",
    "        ax[i].legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e93708b-730d-4b7b-959c-150274f3480a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Final Model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87400c32-63de-40c7-ba51-473811334b03",
   "metadata": {},
   "source": [
    "## Displaying Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73c738b4-33b4-4ee0-83ef-a4db05a4b52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "model = keras.models.load_model('3d_image_classification_relu.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da1916b-d550-492e-a473-333f2dd46c88",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1\n",
      "Data Loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99       506\n",
      "        True       0.99      0.99      0.99       542\n",
      "\n",
      "    accuracy                           0.99      1048\n",
      "   macro avg       0.99      0.99      0.99      1048\n",
      "weighted avg       0.99      0.99      0.99      1048\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.97      0.99       555\n",
      "        True       0.97      1.00      0.98       493\n",
      "\n",
      "    accuracy                           0.99      1048\n",
      "   macro avg       0.99      0.99      0.99      1048\n",
      "weighted avg       0.99      0.99      0.99      1048\n",
      "\n",
      "Fold:2\n",
      "Data Loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99       480\n",
      "        True       0.99      0.99      0.99       569\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.99      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       581\n",
      "        True       0.97      1.00      0.99       468\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.99      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "Fold:3\n",
      "Data Loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99       509\n",
      "        True       0.99      0.99      0.99       540\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.99      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       551\n",
      "        True       0.98      1.00      0.99       498\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.99      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "Fold:4\n",
      "Data Loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99       491\n",
      "        True       0.99      0.99      0.99       558\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.99      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       570\n",
      "        True       0.97      1.00      0.98       479\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.98      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "Fold:5\n",
      "Data Loaded\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.99      0.99       490\n",
      "        True       0.99      0.99      0.99       559\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.99      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      0.98      0.99       571\n",
      "        True       0.97      1.00      0.98       478\n",
      "\n",
      "    accuracy                           0.99      1049\n",
      "   macro avg       0.98      0.99      0.99      1049\n",
      "weighted avg       0.99      0.99      0.99      1049\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "skf = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "skf.get_n_splits(X, y)\n",
    "fold = 0\n",
    "\n",
    "for train_index, test_index in skf.split(X):\n",
    "    fold+=1\n",
    "    print(\"Fold:\"+str(fold))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    iteration=0\n",
    "\n",
    "    outfileTrain = \"Fold \" + str(fold) + \" Checkpoints.pkl\"\n",
    "    outfileTest = \"Fold \" + str(fold) + \" Checkpoints test set.pkl\"\n",
    "\n",
    "    with open(outfileTrain, \"rb\") as f:\n",
    "        [X_train_data, abc] = pickle.load(f)\n",
    "    with open(outfileTest, \"rb\") as f:\n",
    "        [X_valid_data] = pickle.load(f)\n",
    "    print(\"Data Loaded\")\n",
    "    \n",
    "    X_pred = model.predict(tf.expand_dims(X_train_data, axis=4))\n",
    "    Face_pred = []\n",
    "    Brain_pred = []\n",
    "    Face_true = []\n",
    "    Brain_true = []\n",
    "    \n",
    "    len(Face_pred)\n",
    "    dataset_i = 0\n",
    "    for single_prediction in X_pred:\n",
    "        Face_pred.append(single_prediction[0]>0.5)\n",
    "        Brain_pred.append(single_prediction[1]>0.5)\n",
    "        \n",
    "        Face_true.append(y_train[dataset_i][0])\n",
    "        Brain_true.append(y_train[dataset_i][1])\n",
    "        dataset_i+=1\n",
    "    \n",
    "    print(classification_report(Face_true, Face_pred))\n",
    "    print(classification_report(Brain_true, Brain_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b5fd49-c2f8-4f38-a1ca-c44f7a94576d",
   "metadata": {},
   "source": [
    "### Predict with a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dcc47d-e55a-4064-bb54-2b08611541d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "model = keras.models.load_model(\"3d_image_classification_relu.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c009be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = process_scan('../dataset/files/IXI002-Guys-0828-T1_bse_less_s5_r1.nii.gz')\n",
    "file = tf.expand_dims(file, axis=0)\n",
    "\n",
    "prediction = model.predict(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a948819-2207-40ef-b4fb-7196b1fbec4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face Recognizable: True\n",
      "Brain Feature loss: False\n"
     ]
    }
   ],
   "source": [
    "print(\"Face Recognizable:\",prediction[0][0]>0.5)\n",
    "print(\"Brain Feature loss:\",prediction[0][1]>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b9e2c55-c95a-4bee-83d7-87402b0907ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of Face Recognizable 0.99927604\n",
      "Probability of Brain Feature loss: 0.00049352646\n"
     ]
    }
   ],
   "source": [
    "print(\"Probability of Face Recognizable\",prediction[0][0])\n",
    "print(\"Probability of Brain Feature loss:\",prediction[0][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
