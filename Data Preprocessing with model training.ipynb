{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73135157-35fc-4f81-9970-ee1dd2629234",
   "metadata": {},
   "source": [
    "# Data Mining (CpSc 8650) Course Project  Quality Evaluation of Skull Stripped Brain MRI Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faa61c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import random\n",
    "import csv\n",
    "from scipy import ndimage\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.optimizers as optimizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import layers\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fe9c343-c01b-4872-8f82-60c2b9bd19e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if tf.config.list_physical_devices('GPU'):\n",
    "#     physical_devices = tf.config.list_physical_devices('GPU')\n",
    "#     tf.config.experimental.set_memory_growth(physical_devices[0], enable=True)\n",
    "#     tf.config.experimental.set_virtual_device_configuration(physical_devices[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4000)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1cf7e9c-8211-49fc-a4da-c22de3361db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !conda create -n my_environment -y\n",
    "# !source activate my_environment\n",
    "# !pip install sklearn\n",
    "# !pip install nibabel\n",
    "#!wget --no-check-certificate https://dyslexia.computing.clemson.edu/BET_BSE/BET_BSE_DATA.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f38ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and return data \n",
    "def load_data(csv_file_path, test_size = 0.3, x_names = [], y_names = []):\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    Load training data and split it into training and validation set\n",
    "    \"\"\"\n",
    "    print (\"Loading data from: \",csv_file_path)\n",
    "    #reads CSV file into a single dataframe variable\n",
    "    data_df = pd.read_csv(csv_file_path, names=x_names+y_names, skiprows=1)\n",
    "\n",
    "    #yay dataframes, we can select rows and columns by their names\n",
    "    X = data_df[x_names].values\n",
    "    #and our steering commands as our output data\n",
    "    y = data_df[y_names ].values\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def read_nifti_file(filepath):\n",
    "    \"\"\"Read and load volume\"\"\"\n",
    "    # Read file\n",
    "    scan = nib.load(filepath)\n",
    "    # Get raw data\n",
    "    scan = scan.get_fdata()\n",
    "    return scan\n",
    "\n",
    "\n",
    "def normalize(volume):\n",
    "    \"\"\"Normalize the volume\"\"\"\n",
    "    min = -1000\n",
    "    max = 400\n",
    "    volume[volume < min] = min\n",
    "    volume[volume > max] = max\n",
    "    volume = (volume - min) / (max - min)\n",
    "    volume = volume.astype(\"float32\")\n",
    "    return volume\n",
    "\n",
    "\n",
    "def resize_volume(img):\n",
    "    \"\"\"Resize across z-axis\"\"\"\n",
    "    # Set the desired depth\n",
    "    desired_depth = 64\n",
    "    desired_width = 128\n",
    "    desired_height = 128\n",
    "    # Get current depth\n",
    "    current_depth = img.shape[-1]\n",
    "    current_width = img.shape[0]\n",
    "    current_height = img.shape[1]\n",
    "    # Compute depth factor\n",
    "    depth = current_depth / desired_depth\n",
    "    width = current_width / desired_width\n",
    "    height = current_height / desired_height\n",
    "    depth_factor = 1 / depth\n",
    "    width_factor = 1 / width\n",
    "    height_factor = 1 / height\n",
    "    # Rotate\n",
    "    img = ndimage.rotate(img, 90, reshape=False)\n",
    "    # Resize across z-axis\n",
    "    img = ndimage.zoom(img, (width_factor, height_factor, depth_factor), order=1)\n",
    "    return img\n",
    "\n",
    "def process_scan(path):\n",
    "    global iteration\n",
    "    \"\"\"Read and resize volume\"\"\"\n",
    "    # Read scan\n",
    "    volume = read_nifti_file(path)\n",
    "    # Normalize\n",
    "    volume = normalize(volume)\n",
    "    #Resize width, height and depth\n",
    "    volume = resize_volume(volume)\n",
    "    iteration+=1\n",
    "    print(iteration)\n",
    "    return volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68a0d543-e1dc-4e17-bcaa-84de77a0b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def rotate(volume):\n",
    "    \"\"\"Rotate the volume by a few degrees\"\"\"\n",
    "\n",
    "    def scipy_rotate(volume):\n",
    "        # define some rotation angles\n",
    "        angles = [0,60,-60,-90,90,180,-180]\n",
    "        # pick angles at random\n",
    "        angle = random.choice(angles)\n",
    "        # rotate volume\n",
    "        volume = ndimage.rotate(volume, angle, reshape=False)\n",
    "        volume[volume < 0] = 0\n",
    "        volume[volume > 1] = 1\n",
    "        return volume\n",
    "\n",
    "    augmented_volume = tf.numpy_function(scipy_rotate, [volume], tf.float32)\n",
    "    return augmented_volume\n",
    "\n",
    "\n",
    "def train_preprocessing(volume, labels):\n",
    "    \"\"\"Process training data by rotating and adding a channel.\"\"\"\n",
    "    # Rotate volume\n",
    "    volume = rotate(volume)\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, labels\n",
    "\n",
    "\n",
    "def validation_preprocessing(volume, label):\n",
    "    \"\"\"Process validation data by only adding a channel.\"\"\"\n",
    "    volume = tf.expand_dims(volume, axis=3)\n",
    "    return volume, label\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3db25cb-dbba-4d77-b1dc-23effcff6aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from:  /home/rkaranj/Data Mining/New/dataset/Label_file.csv\n"
     ]
    }
   ],
   "source": [
    "label_file_path = os.path.join(os.path.abspath(os.pardir), \"dataset\", \"Label_file.csv\")\n",
    "X, y = load_data(label_file_path, x_names = [\"Filename\"], y_names = [\"Recognizable-Facial-Feature\",\"Brain-Feature-Loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87860c1b-3377-4c3f-baab-212b27c07585",
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y=='Yes'] = 1\n",
    "y[y=='No'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75b7c0ca-9bc3-41e5-8123-79a73457a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(width=128, height=128, depth=64):\n",
    "        \"\"\"Build a 3D convolutional neural network model.\"\"\"\n",
    "\n",
    "        inputs = keras.Input((width, height, depth, 1))\n",
    "\n",
    "        x = layers.Conv3D(filters=16, kernel_size=3, activation=\"relu\")(inputs)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "        x = layers.MaxPool3D(pool_size=2)(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.5)(x)\n",
    "\n",
    "        x = layers.GlobalAveragePooling3D()(x)\n",
    "        x = layers.Dense(units=256, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "\n",
    "        outputs = layers.Dense(units=2, activation=\"sigmoid\")(x)\n",
    "\n",
    "        # Define the model.\n",
    "        model = keras.Model(inputs, outputs, name=\"3dcnn\")\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335dd5c8-7e11-4c20-a635-4f93b8c812e2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold:1\n",
      "Data Loaded\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 131s 3s/step - loss: 0.6059 - acc: 0.7824 - val_loss: 0.7390 - val_acc: 0.4221\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.5348 - acc: 0.9046 - val_loss: 0.8060 - val_acc: 0.4221\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.4706 - acc: 0.9361 - val_loss: 0.8861 - val_acc: 0.4221\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.4241 - acc: 0.9504 - val_loss: 0.9744 - val_acc: 0.4221\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.3853 - acc: 0.9656 - val_loss: 1.0603 - val_acc: 0.4221\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.3457 - acc: 0.9695 - val_loss: 1.1476 - val_acc: 0.4221\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 130s 3s/step - loss: 0.3306 - acc: 0.9685 - val_loss: 1.2088 - val_acc: 0.4221\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.3010 - acc: 0.9733 - val_loss: 1.2494 - val_acc: 0.4221\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.2854 - acc: 0.9752 - val_loss: 1.2683 - val_acc: 0.4221\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.2596 - acc: 0.9771 - val_loss: 1.2638 - val_acc: 0.4221\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.2524 - acc: 0.9742 - val_loss: 1.2004 - val_acc: 0.4221\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.2396 - acc: 0.9790 - val_loss: 1.1382 - val_acc: 0.4221\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.2269 - acc: 0.9771 - val_loss: 1.0698 - val_acc: 0.4221\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.2132 - acc: 0.9800 - val_loss: 1.0242 - val_acc: 0.4221\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.2077 - acc: 0.9771 - val_loss: 0.9826 - val_acc: 0.4221\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1999 - acc: 0.9761 - val_loss: 0.9542 - val_acc: 0.4221\n",
      "Fold:2\n",
      "Data Loaded\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 131s 3s/step - loss: 0.1802 - acc: 0.9819 - val_loss: 0.7594 - val_acc: 0.5191\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1666 - acc: 0.9838 - val_loss: 0.7466 - val_acc: 0.5191\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1643 - acc: 0.9800 - val_loss: 0.7432 - val_acc: 0.5191\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1585 - acc: 0.9819 - val_loss: 0.7443 - val_acc: 0.5191\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1500 - acc: 0.9800 - val_loss: 0.6991 - val_acc: 0.5191\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1367 - acc: 0.9809 - val_loss: 0.6823 - val_acc: 0.5191\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1374 - acc: 0.9838 - val_loss: 0.6865 - val_acc: 0.5191\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1296 - acc: 0.9819 - val_loss: 0.6133 - val_acc: 0.5267\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1202 - acc: 0.9828 - val_loss: 0.5106 - val_acc: 0.5954\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1229 - acc: 0.9809 - val_loss: 0.4686 - val_acc: 0.6412\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1183 - acc: 0.9828 - val_loss: 0.4888 - val_acc: 0.6298\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1157 - acc: 0.9800 - val_loss: 0.4914 - val_acc: 0.6260\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1090 - acc: 0.9847 - val_loss: 0.4769 - val_acc: 0.6565\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1023 - acc: 0.9819 - val_loss: 0.4107 - val_acc: 0.7366\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1006 - acc: 0.9819 - val_loss: 0.3416 - val_acc: 0.8168\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0980 - acc: 0.9847 - val_loss: 0.3183 - val_acc: 0.8511\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1057 - acc: 0.9771 - val_loss: 0.2848 - val_acc: 0.8893\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0958 - acc: 0.9828 - val_loss: 0.2979 - val_acc: 0.8817\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0988 - acc: 0.9819 - val_loss: 0.2595 - val_acc: 0.9084\n",
      "Epoch 20/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0905 - acc: 0.9828 - val_loss: 0.2327 - val_acc: 0.9160\n",
      "Epoch 21/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0850 - acc: 0.9838 - val_loss: 0.2385 - val_acc: 0.9122\n",
      "Epoch 22/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0979 - acc: 0.9800 - val_loss: 0.1782 - val_acc: 0.9466\n",
      "Epoch 23/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0907 - acc: 0.9809 - val_loss: 0.2129 - val_acc: 0.9237\n",
      "Epoch 24/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0935 - acc: 0.9838 - val_loss: 0.2056 - val_acc: 0.9275\n",
      "Epoch 25/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0949 - acc: 0.9809 - val_loss: 0.1849 - val_acc: 0.9351\n",
      "Epoch 26/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0890 - acc: 0.9800 - val_loss: 0.2044 - val_acc: 0.9237\n",
      "Epoch 27/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0870 - acc: 0.9838 - val_loss: 0.1845 - val_acc: 0.9351\n",
      "Epoch 28/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0777 - acc: 0.9819 - val_loss: 0.1810 - val_acc: 0.9351\n",
      "Epoch 29/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0835 - acc: 0.9809 - val_loss: 0.1460 - val_acc: 0.9580\n",
      "Epoch 30/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0841 - acc: 0.9838 - val_loss: 0.1320 - val_acc: 0.9618\n",
      "Epoch 31/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0758 - acc: 0.9838 - val_loss: 0.1385 - val_acc: 0.9618\n",
      "Epoch 32/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0776 - acc: 0.9838 - val_loss: 0.1381 - val_acc: 0.9580\n",
      "Epoch 33/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0739 - acc: 0.9828 - val_loss: 0.1413 - val_acc: 0.9580\n",
      "Epoch 34/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0839 - acc: 0.9790 - val_loss: 0.1280 - val_acc: 0.9656\n",
      "Epoch 35/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0751 - acc: 0.9828 - val_loss: 0.1453 - val_acc: 0.9580\n",
      "Epoch 36/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0726 - acc: 0.9828 - val_loss: 0.1375 - val_acc: 0.9580\n",
      "Epoch 37/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0705 - acc: 0.9857 - val_loss: 0.1276 - val_acc: 0.9618\n",
      "Epoch 38/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0774 - acc: 0.9819 - val_loss: 0.1156 - val_acc: 0.9695\n",
      "Epoch 39/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0707 - acc: 0.9828 - val_loss: 0.1194 - val_acc: 0.9656\n",
      "Epoch 40/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0788 - acc: 0.9819 - val_loss: 0.1228 - val_acc: 0.9656\n",
      "Epoch 41/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0696 - acc: 0.9847 - val_loss: 0.1100 - val_acc: 0.9733\n",
      "Epoch 42/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0699 - acc: 0.9819 - val_loss: 0.1012 - val_acc: 0.9733\n",
      "Epoch 43/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0709 - acc: 0.9838 - val_loss: 0.1051 - val_acc: 0.9733\n",
      "Epoch 44/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0695 - acc: 0.9819 - val_loss: 0.1020 - val_acc: 0.9733\n",
      "Epoch 45/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0781 - acc: 0.9800 - val_loss: 0.0960 - val_acc: 0.9771\n",
      "Epoch 46/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0724 - acc: 0.9800 - val_loss: 0.0903 - val_acc: 0.9771\n",
      "Epoch 47/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0624 - acc: 0.9819 - val_loss: 0.0919 - val_acc: 0.9771\n",
      "Epoch 48/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0722 - acc: 0.9790 - val_loss: 0.0902 - val_acc: 0.9771\n",
      "Epoch 49/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0817 - acc: 0.9762 - val_loss: 0.0890 - val_acc: 0.9771\n",
      "Epoch 50/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0737 - acc: 0.9809 - val_loss: 0.0895 - val_acc: 0.9771\n",
      "Epoch 51/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0706 - acc: 0.9800 - val_loss: 0.0882 - val_acc: 0.9771\n",
      "Epoch 52/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0703 - acc: 0.9809 - val_loss: 0.0887 - val_acc: 0.9771\n",
      "Epoch 53/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0684 - acc: 0.9819 - val_loss: 0.0876 - val_acc: 0.9771\n",
      "Epoch 54/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0634 - acc: 0.9867 - val_loss: 0.0858 - val_acc: 0.9771\n",
      "Epoch 55/100\n",
      "44/44 [==============================] - 130s 3s/step - loss: 0.0642 - acc: 0.9809 - val_loss: 0.0925 - val_acc: 0.9809\n",
      "Epoch 56/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0666 - acc: 0.9809 - val_loss: 0.0873 - val_acc: 0.9771\n",
      "Epoch 57/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0613 - acc: 0.9847 - val_loss: 0.0851 - val_acc: 0.9771\n",
      "Epoch 58/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0611 - acc: 0.9838 - val_loss: 0.0835 - val_acc: 0.9771\n",
      "Epoch 59/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0606 - acc: 0.9819 - val_loss: 0.0835 - val_acc: 0.9771\n",
      "Epoch 60/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0696 - acc: 0.9752 - val_loss: 0.0822 - val_acc: 0.9771\n",
      "Epoch 61/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0665 - acc: 0.9828 - val_loss: 0.0823 - val_acc: 0.9771\n",
      "Epoch 62/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0658 - acc: 0.9809 - val_loss: 0.0827 - val_acc: 0.9771\n",
      "Epoch 63/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0636 - acc: 0.9828 - val_loss: 0.0850 - val_acc: 0.9809\n",
      "Epoch 64/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0632 - acc: 0.9828 - val_loss: 0.0843 - val_acc: 0.9771\n",
      "Epoch 65/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0617 - acc: 0.9838 - val_loss: 0.0922 - val_acc: 0.9809\n",
      "Epoch 66/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0543 - acc: 0.9819 - val_loss: 0.1035 - val_acc: 0.9809\n",
      "Epoch 67/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0601 - acc: 0.9838 - val_loss: 0.0976 - val_acc: 0.9809\n",
      "Epoch 68/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0632 - acc: 0.9800 - val_loss: 0.1124 - val_acc: 0.9809\n",
      "Epoch 69/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.0692 - acc: 0.9800 - val_loss: 0.1089 - val_acc: 0.9809\n",
      "Epoch 70/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.0654 - acc: 0.9838 - val_loss: 0.0926 - val_acc: 0.9809\n",
      "Fold:2\n",
      "Data Loaded\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 130s 3s/step - loss: 0.1997 - acc: 0.9600 - val_loss: 0.3994 - val_acc: 0.8664\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1959 - acc: 0.9581 - val_loss: 0.4276 - val_acc: 0.8664\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1836 - acc: 0.9609 - val_loss: 0.4093 - val_acc: 0.8664\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1890 - acc: 0.9590 - val_loss: 0.3532 - val_acc: 0.8702\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1808 - acc: 0.9571 - val_loss: 0.3396 - val_acc: 0.8626\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1897 - acc: 0.9581 - val_loss: 0.3407 - val_acc: 0.8664\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1793 - acc: 0.9571 - val_loss: 0.3428 - val_acc: 0.8588\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 130s 3s/step - loss: 0.1752 - acc: 0.9590 - val_loss: 0.3457 - val_acc: 0.8588\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1745 - acc: 0.9600 - val_loss: 0.3465 - val_acc: 0.8626\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1676 - acc: 0.9600 - val_loss: 0.3474 - val_acc: 0.8702\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1772 - acc: 0.9581 - val_loss: 0.3512 - val_acc: 0.8626\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1711 - acc: 0.9600 - val_loss: 0.3547 - val_acc: 0.8588\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1733 - acc: 0.9590 - val_loss: 0.3722 - val_acc: 0.8664\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1766 - acc: 0.9571 - val_loss: 0.3754 - val_acc: 0.8664\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1675 - acc: 0.9628 - val_loss: 0.3906 - val_acc: 0.8702\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1681 - acc: 0.9600 - val_loss: 0.3742 - val_acc: 0.8626\n",
      "Epoch 17/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1590 - acc: 0.9600 - val_loss: 0.3955 - val_acc: 0.8702\n",
      "Epoch 18/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1662 - acc: 0.9571 - val_loss: 0.3655 - val_acc: 0.8664\n",
      "Epoch 19/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1628 - acc: 0.9590 - val_loss: 0.3724 - val_acc: 0.8435\n",
      "Fold:2\n",
      "Data Loaded\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1507 - acc: 0.9676 - val_loss: 0.3415 - val_acc: 0.9008\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1551 - acc: 0.9619 - val_loss: 0.3408 - val_acc: 0.9008\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1603 - acc: 0.9619 - val_loss: 0.3538 - val_acc: 0.8893\n",
      "Epoch 4/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1404 - acc: 0.9657 - val_loss: 0.3694 - val_acc: 0.8779\n",
      "Epoch 5/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1525 - acc: 0.9657 - val_loss: 0.3988 - val_acc: 0.8435\n",
      "Epoch 6/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1506 - acc: 0.9657 - val_loss: 0.4054 - val_acc: 0.8397\n",
      "Epoch 7/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1459 - acc: 0.9657 - val_loss: 0.4806 - val_acc: 0.8015\n",
      "Epoch 8/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1502 - acc: 0.9638 - val_loss: 0.5390 - val_acc: 0.7710\n",
      "Epoch 9/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1589 - acc: 0.9647 - val_loss: 0.4542 - val_acc: 0.8130\n",
      "Epoch 10/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1506 - acc: 0.9638 - val_loss: 0.4905 - val_acc: 0.7977\n",
      "Epoch 11/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1513 - acc: 0.9628 - val_loss: 0.4839 - val_acc: 0.7939\n",
      "Epoch 12/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1587 - acc: 0.9619 - val_loss: 0.4162 - val_acc: 0.8244\n",
      "Epoch 13/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1446 - acc: 0.9638 - val_loss: 0.4285 - val_acc: 0.8130\n",
      "Epoch 14/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1493 - acc: 0.9638 - val_loss: 0.3919 - val_acc: 0.8473\n",
      "Epoch 15/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1581 - acc: 0.9657 - val_loss: 0.4194 - val_acc: 0.8168\n",
      "Epoch 16/100\n",
      "44/44 [==============================] - 128s 3s/step - loss: 0.1411 - acc: 0.9666 - val_loss: 0.4776 - val_acc: 0.7977\n",
      "Fold:2\n",
      "Data Loaded\n",
      "<PrefetchDataset element_spec=(TensorSpec(shape=<unknown>, dtype=tf.float32, name=None), TensorSpec(shape=(None, 2), dtype=tf.float32, name=None))>\n",
      "Epoch 1/100\n",
      "44/44 [==============================] - 130s 3s/step - loss: 0.1102 - acc: 0.9743 - val_loss: 0.2478 - val_acc: 0.9008\n",
      "Epoch 2/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1108 - acc: 0.9724 - val_loss: 0.3542 - val_acc: 0.8282\n",
      "Epoch 3/100\n",
      "44/44 [==============================] - 129s 3s/step - loss: 0.1063 - acc: 0.9743 - val_loss: 0.3297 - val_acc: 0.8282\n",
      "Epoch 4/100\n",
      "13/44 [=======>......................] - ETA: 1:30 - loss: 0.1639 - acc: 0.9615"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "skf = KFold(n_splits=5,shuffle=True,random_state=1)\n",
    "skf.get_n_splits(X, y)\n",
    "i = 0\n",
    "\n",
    "model = get_model(width=128, height=128, depth=64)\n",
    "for train_index, test_index in skf.split(X):\n",
    "    i+=1\n",
    "    print(\"Fold:\"+str(i))\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    iteration=0\n",
    "    #print(len(y_train), len(y_test))\n",
    "    # X_train_data = np.array([process_scan(os.path.join(os.path.abspath(os.pardir), \"dataset\", \"files\", path[0]+\".gz\")) for path in X_train])\n",
    "    # X_valid_data = np.array([process_scan(os.path.join(os.path.abspath(os.pardir), \"dataset\", \"files\", path[0]+\".gz\")) for path in X_test])\n",
    "    \n",
    "    outfileTrain = \"Fold \" + str(i) + \" Checkpoints.pkl\"\n",
    "    outfileTest = \"Fold \" + str(i) + \" Checkpoints test set.pkl\"\n",
    "    #X_train_valid = [X_valid_data]\n",
    "#     with open(outfile, \"wb\") as f:\n",
    "#         pickle.dump(X_train_valid, f)\n",
    "    #print(len(X_valid_data), len(y_test))\n",
    "    # Load the objects back\n",
    "    with open(outfileTrain, \"rb\") as f:\n",
    "        [X_train_data, abc] = pickle.load(f)\n",
    "        \n",
    "    with open(outfileTest, \"rb\") as f:\n",
    "        [X_valid_data] = pickle.load(f)\n",
    "    print(\"Data Loaded\")\n",
    "    # Define data loaders.\n",
    "    train_loader = tf.data.Dataset.from_tensor_slices((X_train_data,y_train.astype(\"float32\")))\n",
    "    validation_loader = tf.data.Dataset.from_tensor_slices((X_valid_data, y_test.astype(\"float32\")))\n",
    "    \n",
    "    batch_size = 24\n",
    "    # Augment the on the fly during training.\n",
    "    train_dataset = (\n",
    "        train_loader.shuffle(len(X_train_data))\n",
    "        .map(train_preprocessing)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "    \n",
    "    # Only rescale.\n",
    "    validation_dataset = (\n",
    "        validation_loader.shuffle(len(X_valid_data))\n",
    "        .map(validation_preprocessing)\n",
    "        .batch(batch_size)\n",
    "        .prefetch(2)\n",
    "    )\n",
    "    # Build model.\n",
    "    print(train_dataset)\n",
    "    # Compile model.\n",
    "    initial_learning_rate = 0.00001\n",
    "    lr_schedule = optimizers.schedules.ExponentialDecay(\n",
    "        initial_learning_rate, decay_steps=100000, decay_rate=0.96, staircase=True\n",
    "    )\n",
    "    model.compile(\n",
    "        loss=\"binary_crossentropy\",\n",
    "        optimizer=optimizers.Adam(learning_rate=lr_schedule),\n",
    "        metrics=[\"acc\"],\n",
    "    )\n",
    "\n",
    "    \n",
    "    # Define callbacks.\n",
    "    checkpoint_cb = keras.callbacks.ModelCheckpoint(\n",
    "        \"3d_image_classification_relu.h5\", save_best_only=True\n",
    "    )\n",
    "\n",
    "    early_stopping_cb = keras.callbacks.EarlyStopping(monitor=\"val_acc\", patience=15)\n",
    "\n",
    "    # Train the model, doing validation at the end of each epoch\n",
    "    epochs = 100\n",
    "    model.fit(\n",
    "        train_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=epochs,\n",
    "        callbacks=[checkpoint_cb, early_stopping_cb],\n",
    "    )\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 2, figsize=(20, 3))\n",
    "    ax = ax.ravel()\n",
    "\n",
    "    for i, metric in enumerate([\"acc\", \"loss\"]):\n",
    "        ax[i].plot(model.history.history[metric])\n",
    "        ax[i].plot(model.history.history[\"val_\" + metric])\n",
    "        ax[i].set_title(\"Model {}\".format(metric))\n",
    "        ax[i].set_xlabel(\"epochs\")\n",
    "        ax[i].set_ylabel(metric)\n",
    "        ax[i].legend([\"train\", \"val\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc52fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "model = keras.models.load_model('3d_image_classification_relu.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87400c32-63de-40c7-ba51-473811334b03",
   "metadata": {},
   "source": [
    "## Displaying Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ba5ce66",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/mz/wm9wkty5247g4645dd_895l80000gp/T/ipykernel_47173/1474995535.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dimension of the CT scan is:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gray\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_data' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "image = X_train_data[0]\n",
    "print(\"Dimension of the CT scan is:\", image.shape)\n",
    "plt.imshow(np.squeeze(image[:, :, 30]), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cbe5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_slices(num_rows, num_columns, width, height, data):\n",
    "    \"\"\"Plot a montage of 20 CT slices\"\"\"\n",
    "    data = np.rot90(np.array(data))\n",
    "    data = np.transpose(data)\n",
    "    data = np.reshape(data, (num_rows, num_columns, width, height))\n",
    "    rows_data, columns_data = data.shape[0], data.shape[1]\n",
    "    heights = [slc[0].shape[0] for slc in data]\n",
    "    widths = [slc.shape[1] for slc in data[0]]\n",
    "    fig_width = 12.0\n",
    "    fig_height = fig_width * sum(heights) / sum(widths)\n",
    "    f, axarr = plt.subplots(\n",
    "        rows_data,\n",
    "        columns_data,\n",
    "        figsize=(fig_width, fig_height),\n",
    "        gridspec_kw={\"height_ratios\": heights},\n",
    "    )\n",
    "    for i in range(rows_data):\n",
    "        for j in range(columns_data):\n",
    "            axarr[i, j].imshow(data[i][j], cmap=\"gray\")\n",
    "            axarr[i, j].axis(\"off\")\n",
    "    plt.subplots_adjust(wspace=0, hspace=0, left=0, right=1, bottom=0, top=1)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Visualize montage of slices.\n",
    "# 4 rows and 10 columns for 100 slices of the CT scan.\n",
    "plot_slices(4, 10, 128, 128, image[:, :, :40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c559d9f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e6b5fd49-c2f8-4f38-a1ca-c44f7a94576d",
   "metadata": {},
   "source": [
    "### Predict with a image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c009be50",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = process_scan('../dataset/files/IXI002-Guys-0828-T1_bse_less_s5_r1.nii.gz')\n",
    "file = tf.expand_dims(file, axis=0)\n",
    "\n",
    "prediction = model.predict(file)\n",
    "\n",
    "\n",
    "# yes,no = prediction[0] * 100, (1-prediction[0]) *100\n",
    "\n",
    "# print(\"Yes=\",yes[0],\"No=\",no[0])\n",
    "# if(yes[0]>50):\n",
    "#     print(\"Facial Feature Recognizable\")\n",
    "# else:\n",
    "#     print(\"Facial Feature not recognizable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a948819-2207-40ef-b4fb-7196b1fbec4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0][0]>0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b33b64-ff6d-46d0-8e14-846988f46fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0][1]>0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde34007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
